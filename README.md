# exploringFeikskeip

A video demonstration of the project can be viewed here https://vimeo.com/161603215

‘Exploring Feikskeip’ was originally inspired by the question, ‘How long does it take to look at a painting?’. Clearly this is subjective. For some people it may take a few seconds, for others many hours over repeated visits. I was particularly interested in those who describe an ongoing and evolving relationship with a still piece of art, despite the fact it doesn’t change. My goal for the project was to use this evolving relationship as a mechanism for an interactive sound composition, which could in some small way actualise this abstract conversation between viewer and artwork.

I envisioned the project would be an installation whereby the viewer is shown an artwork on a screen, and simultaneously their gaze is tracked and used in an interactive composition which they listen to on headphones whilst looking at the artwork. I was inspired by other artists who have used eye tracking in order to create interactive installations such as Jonas Lund’s ‘VIP’ (2014) and Golan Levin’s ‘Eyeshine’ (2011).

The artwork on display in my installation was obviously a key aspect of the overall piece. I had found an interesting article related to gaze tracking whilst viewing art (http://blog.art21.org/2013/01/07/tracking-the-gaze/#.VwJpGLRRdEf), which suggested that figurative paintings often have similar visual pathways among viewers, as they try to construct a narrative by focusing on recognisable forms (e.g.: faces, objects). For abstract paintings however, whilst the viewer’s gaze is often attracted to areas of salience first, it becomes a more personal pathway the longer the painting is viewed. This idea of each viewer’s gaze path becoming a personal journey appealed to me as it would allow my composition to also become personal and slightly different for each viewer.

I decided to approach Rebecca Harris, an artist whose work I admire, about the possibility of using one of her abstract digital collages for the artwork on display. She kindly accepted this collaboration, and I chose one of her pieces ‘Feikskeip 3’.

This artwork was exactly the kind I wanted to work with, an abstract piece with a lot of detail but nothing that clearly took center stage. The kind of artwork you could spend a long time examining and continue to find new details. I was drawn to the ways in which the colours and shapes are tangled and stitched together with a glitchy and handcrafted aesthetic, which felt to me almost like a digital forest or jungle. I used this idea as inspiration to create the interactive composition, taking the form of an evolving sonic collage that uses a large number of short digitally manipulated field recordings.

In order to create the composition, I used an interactive genetic algorithm which divided ‘Feikskeip 3’ into a grid of 50 rectangles. Each rectangle represents an instance of a class called SoundSquare, each with its own DNA genotype. The genotype determines the phenotype of the SoundSquare, an array of integers that can be used as a reference index for choosing a sequence of 5 of my audio samples (out of a selection of 50), to be triggered at regular time intervals.

After this sequence has been played, a process of natural selection is used to generate a new population of SoundSquares based on the genes of their parents. The likelihood of certain genes being prevalent in the overall population is a result of the parent’s fitness score, whereby fitter parents are more likely to breed and pass on their genes. Fitness is determined by how long the viewer’s gaze remains on a specific SoundSquare. In this way, the sonic collage can evolve so that the same sounds are likely to come back if the viewer’s gaze remains in a certain area. However, in order for new sounds to be able to enter into the sequence when the viewer's gaze moves into a new area, the genetic algorithm uses gene mutation. This mutation means that for each gene there is a small probability that the gene will 'mutate' into a random genotype unrelated to either of the parents. As a result, the population will never reach a definitive point where everyone has the same genes, meaning that new sounds can continue to enter the sequence.

For the gaze tracking, I used an Eyetribe infra-red camera which could be placed with the display screen and obtain data regarding the viewer’s eye movement. For the audio, I used Mick Grierson’s ofxMaxim addon: https://github.com/micknoise/Maximilian.

The physical installation is set up so the viewer sits in front of a black screen and wears a pair of headphones. Once the camera detects the viewer, ‘Feikskeip 3’ is displayed and the composition begins. After the viewer has finished and moves away, the image will disappear and the composition will stop. 

